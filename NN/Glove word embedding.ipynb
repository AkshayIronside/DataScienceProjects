{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9393818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras.models\n",
    "from PIL import Image\n",
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Conv2D ,MaxPool2D, Flatten, Dense, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, BatchNormalization;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a094561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors. \n",
      "Found 400000 word vectors,\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors. ')\n",
    "embeddings_index = {}\n",
    "f = open(r'F:\\DS\\KAGGLE DATASET\\Neural Network\\glove.6B\\glove.6B.100d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors,' %len(embeddings_index))\n",
    "\n",
    "## this glove.6B.100d.txt file has 400000 words and their corresponding embedding\n",
    "\n",
    "# i am using the 100d... it can be 50 d ...200d... etc... D stands for dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d23a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data set is also downloaded from this webside\n",
    "# http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# https://nlp.stanford.edu/projects/glove this side will give the description about the glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18454be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "u = embeddings_index['newton']\n",
    "norm_u = np.linalg.norm(u)  ## normalizing the vector (take square root of the sum of square )\n",
    "similarity = []\n",
    "for word in embeddings_index.keys(): # it searches all the words in the corpus and calculates sosine similarity\n",
    "    v = embeddings_index[word]\n",
    "    cosine = np.dot(u , v)/norm_u / np.linalg.norm(v)  # calculating the cosine similarity\n",
    "    similarity.append((word,cosine))\n",
    "    \n",
    "print(len(similarity))\n",
    "\n",
    "# for cosine similarity refer this link https://en.wikipedia.org/wiki/cosine_similarity\n",
    "\n",
    "# we take cosine similarity and not other similarity because if two woeds are similer they would be parallel\n",
    "# to each other and so the angle between them would be zero and cos0 is 1 and we will have highest score \n",
    "\n",
    "# if 2 words are opposite to each other they would be perpendicular (90 degree), cos90 will be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4c45068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('newton', 0.9999999),\n",
       " ('webster', 0.6259993),\n",
       " ('hayes', 0.5794195),\n",
       " ('carroll', 0.5742025),\n",
       " ('cambridge', 0.5738973),\n",
       " ('mansfield', 0.57085687),\n",
       " ('russell', 0.5666629),\n",
       " ('preston', 0.5658804),\n",
       " ('fisher', 0.5588769),\n",
       " ('compton', 0.5579355),\n",
       " ('mason', 0.55375606),\n",
       " ('davidson', 0.54781556),\n",
       " ('irvine', 0.54672307),\n",
       " ('andover', 0.5434853),\n",
       " ('maynard', 0.54338926),\n",
       " ('logan', 0.53622144),\n",
       " ('john', 0.53572375),\n",
       " ('ray', 0.5345969),\n",
       " ('merton', 0.53349715),\n",
       " ('bedford', 0.53338236),\n",
       " ('james', 0.5322112),\n",
       " ('weston', 0.5319792),\n",
       " ('whitehead', 0.5300733),\n",
       " ('smith', 0.5297747),\n",
       " ('maxwell', 0.529762),\n",
       " ('walton', 0.52928185),\n",
       " ('ely', 0.5269908),\n",
       " ('robinson', 0.5268592),\n",
       " ('conway', 0.52657735),\n",
       " ('lawrence', 0.52649075),\n",
       " ('adams', 0.5260335),\n",
       " ('jefferson', 0.5247552),\n",
       " ('oxford', 0.52281827),\n",
       " ('hamilton', 0.52179927),\n",
       " ('bloomfield', 0.5215087),\n",
       " ('harrison', 0.5213449),\n",
       " ('jacobs', 0.5199517),\n",
       " ('dixon', 0.519036),\n",
       " ('coleman', 0.51863146),\n",
       " ('moore', 0.5180319),\n",
       " ('chandler', 0.5167397),\n",
       " ('arthur', 0.5165642),\n",
       " ('montgomery', 0.516073),\n",
       " ('augustine', 0.5151988),\n",
       " ('rutherford', 0.51395345),\n",
       " ('gardner', 0.5134871),\n",
       " ('fuller', 0.5131929),\n",
       " ('gould', 0.5127945),\n",
       " ('lincoln', 0.51197934),\n",
       " ('isaac', 0.51159036),\n",
       " ('aycliffe', 0.5090265),\n",
       " ('clifton', 0.50790644),\n",
       " ('barton', 0.50787264),\n",
       " ('bentley', 0.5078423),\n",
       " ('wallace', 0.5074129),\n",
       " ('bradford', 0.50660795),\n",
       " ('thomas', 0.5065229),\n",
       " ('harris', 0.50651735),\n",
       " ('anderson', 0.5058162),\n",
       " ('taylor', 0.50532466),\n",
       " ('hawthorne', 0.50521594),\n",
       " ('dudley', 0.5043871),\n",
       " ('franklin', 0.50266486),\n",
       " ('walden', 0.50129086),\n",
       " ('cooper', 0.49878493),\n",
       " ('jasper', 0.49759796),\n",
       " ('durham', 0.49729332),\n",
       " ('martin', 0.49707472),\n",
       " ('chapman', 0.4950718),\n",
       " ('ann', 0.49408942),\n",
       " ('william', 0.49380258),\n",
       " ('winchester', 0.49324775),\n",
       " ('henderson', 0.4930897),\n",
       " ('henry', 0.49307996),\n",
       " ('quincy', 0.49250165),\n",
       " ('dickinson', 0.4921184),\n",
       " ('parker', 0.4919609),\n",
       " ('berkeley', 0.49152502),\n",
       " ('sutton', 0.49085897),\n",
       " ('bates', 0.49053797),\n",
       " ('st', 0.49053782),\n",
       " ('hastings', 0.49041164),\n",
       " ('somerville', 0.49028215),\n",
       " ('gresham', 0.48977762),\n",
       " ('mills', 0.48949647),\n",
       " ('milton', 0.48879975),\n",
       " ('marshall', 0.48850864),\n",
       " ('andrews', 0.48812094),\n",
       " ('chester', 0.4874805),\n",
       " ('baker', 0.4869798),\n",
       " ('heath', 0.48685902),\n",
       " ('emerson', 0.4841547),\n",
       " ('barnes', 0.4839548),\n",
       " ('waltham', 0.48363203),\n",
       " ('middleton', 0.4833359),\n",
       " ('richmond', 0.48304263),\n",
       " ('grammar', 0.48277077),\n",
       " ('calvin', 0.4826351),\n",
       " ('murphy', 0.4824409),\n",
       " ('bristol', 0.48234326)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(similarity,reverse = True, key = lambda y : y[1])[:100]  #  seeing top 100 matching to science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc79b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
